# ç¬¬ä¸€æ­¥ï¼šé…ç½®å›½å†…é•œåƒ + å¯¼å…¥æ ¸å¿ƒåº“
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from transformers import AutoTokenizer, AutoModel
import torch

# ç¬¬äºŒæ­¥ï¼šåŠ è½½bge-small-zhæ¨¡å‹ï¼ˆå›ºå®šå†™æ³•ï¼‰
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-small-zh", use_fast=False)
model = AutoModel.from_pretrained("BAAI/bge-small-zh")

# ç¬¬ä¸‰æ­¥ï¼šåŸºç¡€å‘é‡ç”Ÿæˆå‡½æ•°ï¼ˆæ ¸å¿ƒä¸å˜ï¼ŒåŠ æ³¨é‡Šï¼‰
def get_text_embedding(texts):
    """
    ç”Ÿæˆæ–‡æœ¬çš„å½’ä¸€åŒ–è¯­ä¹‰å‘é‡ï¼ˆbge-small-zhæ ¸å¿ƒï¼‰
    :param texts: å•ä¸ªæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :return: å½’ä¸€åŒ–åçš„å‘é‡ï¼ˆshape: [æ–‡æœ¬æ•°, 768]ï¼‰
    """
    if isinstance(texts, str):
        texts = [texts]
    
    # ç¼–ç æ–‡æœ¬ï¼ˆbge-small-zhæ ‡å‡†é…ç½®ï¼‰
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    
    # ç”Ÿæˆå‘é‡ï¼ˆç¦ç”¨æ¢¯åº¦ï¼Œæå‡é€Ÿåº¦ï¼‰
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :]  # å–[CLS]ä½ç½®å‘é‡
    
    # å‘é‡å½’ä¸€åŒ–ï¼ˆå¿…é¡»ï¼å¦åˆ™matmulä¸æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
    return embeddings

# ç¬¬å››æ­¥ï¼šè¿›é˜¶1 - å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„é—®ç­”ï¼ˆä¿®æ­£ç¬”è¯¯ï¼‰
def rag_qa_with_score(question, threshold=0.5):
    """
    å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„é—®ç­”å‡½æ•°ï¼ˆå·¥ä¸šç•Œæ ‡å‡†å†™æ³•ï¼‰
    :param question: ç”¨æˆ·é—®é¢˜
    :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼æ‹’ç»å›ç­”ï¼‰
    :return: ç­”æ¡ˆ/ä¸çŸ¥é“
    """
    # ç”Ÿæˆé—®é¢˜å‘é‡
    q_embedding = get_text_embedding(question)
    # è®¡ç®—å’Œæ‰€æœ‰çŸ¥è¯†åº“çš„ç›¸ä¼¼åº¦ï¼ˆæ ¸å¿ƒï¼šä¿®æ­£similaritiesâ†’similarity_scoresï¼‰
    similarity_scores = torch.matmul(q_embedding, kb_embeddings.T)
    
    # æ‰“å°åˆ†æ•°ï¼ˆå…³é”®ï¼šè®©ä½ çŸ¥é“ä¸ºä»€ä¹ˆé€‰è¿™æ¡ç­”æ¡ˆï¼‰
    print(f"\nã€{question}ã€‘çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼š")
    for i, score in enumerate(similarity_scores[0]):
        print(f"  - å’ŒçŸ¥è¯†åº“{i+1}çš„ç›¸ä¼¼åº¦ï¼š{score.item():.4f}")
    
    # æ‰¾æœ€é«˜åˆ†å’Œå¯¹åº”ç´¢å¼•
    best_score = similarity_scores.max().item()
    best_idx = torch.argmax(similarity_scores).item()

    # ä½äºé˜ˆå€¼ â†’ æ‹’ç»å›ç­”ï¼ˆé¿å…èƒ¡è¯´ï¼‰
    if best_score < threshold:
        return f"ğŸ¤·â€â™‚ï¸ æˆ‘ä¸çŸ¥é“ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦ä»… {best_score:.4f}ï¼Œä½äºé˜ˆå€¼{threshold}ï¼‰"
    
    # æå–ç­”æ¡ˆ
    best_text = knowledge_base[best_idx]
    answer = best_text.split("ï¼Ÿ")[1].strip()
    return f"âœ… ç­”æ¡ˆï¼š{answer}ï¼ˆç›¸ä¼¼åº¦ï¼š{best_score:.4f}ï¼‰"

# ç¬¬äº”æ­¥ï¼šè¿›é˜¶2 - æ‰¹é‡å‘é‡åŒ–ï¼ˆå¤„ç†å¤§é‡æ•°æ®ï¼‰
def batch_embed(texts, batch_size=2):
    """
    æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆçœŸå®åœºæ™¯å¿…ç”¨ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
    :param texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆå¯ä¸Šåƒ/ä¸Šä¸‡æ¡ï¼‰
    :param batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡æœ¬æ•°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
    :return: æ‰€æœ‰æ–‡æœ¬çš„å‘é‡ï¼ˆshape: [æ€»æ¡æ•°, 768]ï¼‰
    """
    all_embeddings = []
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]  # å–å½“å‰æ‰¹æ¬¡
        batch_emb = get_text_embedding(batch_texts)  # ç”Ÿæˆå½“å‰æ‰¹æ¬¡å‘é‡
        all_embeddings.append(batch_emb)  # å­˜å…¥åˆ—è¡¨
    
    # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„å‘é‡
    return torch.cat(all_embeddings, dim=0)

# ç¬¬å…­æ­¥ï¼šè¿›é˜¶3 - ç®€æ˜“å‘é‡åº“ï¼ˆæ¨¡æ‹ŸFAISS/Chromaï¼‰
class SimpleVectorDB:
    """æ‰‹å†™æœ€ç®€å‘é‡åº“ï¼ˆç†è§£å·¥ä¸šç•Œå‘é‡åº“çš„æ ¸å¿ƒé€»è¾‘ï¼‰"""
    def __init__(self):
        self.texts = []  # å­˜å‚¨åŸå§‹æ–‡æœ¬
        self.embeddings = None  # å­˜å‚¨æ–‡æœ¬å‘é‡
    
    def add_texts(self, texts):
        """æ·»åŠ æ–‡æœ¬åˆ°å‘é‡åº“ï¼ˆå¹¶ç”Ÿæˆå‘é‡ï¼‰"""
        self.texts = texts
        self.embeddings = batch_embed(texts)  # æ‰¹é‡ç”Ÿæˆå‘é‡
    
    def search(self, query, top_k=1):
        """
        è¯­ä¹‰æ£€ç´¢ï¼ˆæ‰¾æœ€ç›¸ä¼¼çš„top_kæ¡ï¼‰
        :param query: ç”¨æˆ·é—®é¢˜
        :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„kæ¡
        :return: æœ€ç›¸ä¼¼çš„æ–‡æœ¬åˆ—è¡¨
        """
        q_emb = get_text_embedding(query)
        # è®¡ç®—ç›¸ä¼¼åº¦
        scores = torch.matmul(q_emb, self.embeddings.T)
        # å–top_kä¸ªæœ€é«˜åˆ†çš„ç´¢å¼•
        top_k_indices = scores.topk(top_k).indices[0].tolist()
        # è¿”å›å¯¹åº”çš„æ–‡æœ¬
        return [self.texts[idx] for idx in top_k_indices]

# ===================== æµ‹è¯•æ‰€æœ‰è¿›é˜¶åŠŸèƒ½ =====================
# 1. æ„å»ºçŸ¥è¯†åº“
knowledge_base = [
    "ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹ï¼Ÿå¤§æ¨¡å‹æ˜¯åŸºäºæµ·é‡æ•°æ®è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤„ç†å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚",
    "å¾®è°ƒéœ€è¦å¤šå°‘æ•°æ®ï¼Ÿè½»é‡çº§å¾®è°ƒåªéœ€å‡ åæ¡æ•°æ®ï¼Œå°±èƒ½è®©æ¨¡å‹é€‚é…ç‰¹å®šä»»åŠ¡ã€‚",
    "GPT2æ¨¡å‹æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼ŸGPT2æ˜¯å°å‹ç”Ÿæˆå¼æ¨¡å‹ï¼Œä½“ç§¯å°ã€è¿è¡Œå¿«ï¼Œé€‚åˆå…¥é—¨å­¦ä¹ ã€‚"
]

# 2. é¢„ç”ŸæˆçŸ¥è¯†åº“å‘é‡ï¼ˆç”¨æ‰¹é‡å‘é‡åŒ–ï¼‰
kb_embeddings = batch_embed(knowledge_base)
print("âœ… çŸ¥è¯†åº“å‘é‡å½¢çŠ¶ï¼š", kb_embeddings.shape)  # è¾“å‡º torch.Size([3, 768])

# 3. æµ‹è¯•è¿›é˜¶1ï¼šå¸¦åˆ†æ•°çš„é—®ç­”
print("=== è¿›é˜¶1ï¼šå¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„é—®ç­” ===")
print(rag_qa_with_score("å¤§æ¨¡å‹ä¸æ˜¯å•¥ï¼Ÿ", threshold=0.5))
print(rag_qa_with_score("GPT2æœ‰å•¥ä¼˜åŠ¿ï¼Ÿ", threshold=0.5))
print(rag_qa_with_score("Pythonæ€ä¹ˆå­¦ï¼Ÿ", threshold=0.5))  # ä½äºé˜ˆå€¼ï¼Œè¿”å›ä¸çŸ¥é“

# 4. æµ‹è¯•è¿›é˜¶3ï¼šç®€æ˜“å‘é‡åº“
print("\n=== è¿›é˜¶3ï¼šç®€æ˜“å‘é‡åº“æ£€ç´¢ ===")
db = SimpleVectorDB()
db.add_texts(knowledge_base)
top1_text = db.search("å¾®è°ƒéœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ")[0]
print("æ£€ç´¢åˆ°çš„æœ€ç›¸ä¼¼æ–‡æœ¬ï¼š", top1_text)