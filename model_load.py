# ç¬¬ä¸€æ­¥ï¼šé…ç½®å›½å†…é•œåƒï¼Œè§£å†³æ¨¡å‹ä¸‹è½½é—®é¢˜
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ç¬¬äºŒæ­¥ï¼šå¯¼å…¥æ ¸å¿ƒåº“
from transformers import AutoTokenizer, AutoModel
import torch

# ç¬¬ä¸‰æ­¥ï¼šåŠ è½½bge-small-zhæ¨¡å‹ï¼ˆæŒ‡å®šä½¿ç”¨æ…¢åˆ†è¯å™¨ï¼Œé¿å…è½¬æ¢æŠ¥é”™ï¼‰
model_name = "BAAI/bge-small-zh"
try:
    # å¼ºåˆ¶ä½¿ç”¨æ…¢åˆ†è¯å™¨ï¼Œé¿å…sentencepieceè½¬æ¢é—®é¢˜
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        use_fast=False  # å…³é”®ï¼šç¦ç”¨fast tokenizerï¼Œå½»åº•è§£å†³è½¬æ¢æŠ¥é”™
    )
    model = AutoModel.from_pretrained(model_name)
    print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼ˆç½‘ç»œä¸‹è½½ï¼‰")
except Exception as e:
    # å…œåº•ï¼šæœ¬åœ°åŠ è½½ï¼ˆå¦‚æœç½‘ç»œä»æœ‰é—®é¢˜ï¼‰
    print(f"âš ï¸  ç½‘ç»œåŠ è½½å¤±è´¥ï¼Œå°è¯•æœ¬åœ°åŠ è½½ï¼š{e}")
    print("ğŸ‘‰ æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° D:\bge-small-zhï¼Œåœ°å€ï¼šhttps://hf-mirror.com/BAAI/bge-small-zh")
    model_path = r"D:\bge-small-zh"
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    model = AutoModel.from_pretrained(model_path)
    print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼ˆæœ¬åœ°åŠ è½½ï¼‰")

# ç¬¬å››æ­¥ï¼šå®šä¹‰å‘é‡ç”Ÿæˆå‡½æ•°ï¼ˆbge-small-zhæ ¸å¿ƒï¼‰
def get_text_embedding(texts):
    """ç”Ÿæˆæ–‡æœ¬çš„å½’ä¸€åŒ–å‘é‡"""
    if isinstance(texts, str):
        texts = [texts]
    
    # ç¼–ç æ–‡æœ¬ï¼ˆbge-small-zhæ ‡å‡†é…ç½®ï¼‰
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    
    # ç”Ÿæˆå‘é‡ï¼ˆç¦ç”¨æ¢¯åº¦ï¼Œæå‡é€Ÿåº¦ï¼‰
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :]  # å–[CLS]ä½ç½®å‘é‡
    
    # å‘é‡å½’ä¸€åŒ–ï¼ˆå¿…åšï¼Œä¿è¯ç›¸ä¼¼åº¦è®¡ç®—å‡†ç¡®ï¼‰
    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
    return embeddings

# ç¬¬äº”æ­¥ï¼šæ„å»ºçŸ¥è¯†åº“+è¯­ä¹‰æ£€ç´¢
knowledge_base = [
    "ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹ï¼Ÿå¤§æ¨¡å‹æ˜¯åŸºäºæµ·é‡æ•°æ®è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤„ç†å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚",
    "å¾®è°ƒéœ€è¦å¤šå°‘æ•°æ®ï¼Ÿè½»é‡çº§å¾®è°ƒåªéœ€å‡ åæ¡æ•°æ®ï¼Œå°±èƒ½è®©æ¨¡å‹é€‚é…ç‰¹å®šä»»åŠ¡ã€‚",
    "GPT2æ¨¡å‹æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼ŸGPT2æ˜¯å°å‹ç”Ÿæˆå¼æ¨¡å‹ï¼Œä½“ç§¯å°ã€è¿è¡Œå¿«ï¼Œé€‚åˆå…¥é—¨å­¦ä¹ ã€‚"
]

# é¢„ç”ŸæˆçŸ¥è¯†åº“å‘é‡
kb_embeddings = get_text_embedding(knowledge_base)


def rag_qa_with_score(question, threshold=0.7):
    q_embedding = get_text_embedding(question)
    similarity_scores = torch.matmul(q_embedding, kb_embeddings.T)
    
    # æŠŠåˆ†æ•°æ‰“å°å‡ºæ¥ï¼ï¼ˆæœ€å…³é”®ï¼‰
    print("ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå’Œ3æ¡çŸ¥è¯†åº“ï¼‰ï¼š", similarity_scores)
    
    best_score = similarity_scores.max().item()
    best_idx = torch.argmax(similarity_scores).item()

    # ä½äºé˜ˆå€¼ â†’ æ‹’ç»å›ç­”
    if best_score < threshold:
        return f"[ä¸çŸ¥é“] æœ€é«˜åˆ†åªæœ‰ {best_score:.2f}"
    
    best_text = knowledge_base[best_idx]
    answer = best_text.split("ï¼Ÿ")[1].strip()
    return answer

# ç¬¬å…­æ­¥ï¼šæµ‹è¯•æ•ˆæœ
print("\n=== BAAI/bge-small-zh é—®ç­”æ•ˆæœ ===")
questions = [
    "å¤§æ¨¡å‹æ˜¯å•¥ï¼Ÿ",
    "å¾®è°ƒéœ€è¦å‡ æ¡æ•°æ®ï¼Ÿ",
    "GPT2æœ‰å•¥ç‰¹ç‚¹ï¼Ÿ",
    "ä»Šå¤©æ™šä¸Šåƒä»€ä¹ˆï¼Ÿ"
]

for q in questions:
    print(f"é—®é¢˜ï¼š{q}")
    print(f"ç­”æ¡ˆï¼š{rag_qa_with_score(q)}\n")

